#This default config file aims to support most popular model providers out of the box

#In general, the model name used by the client will be the same as the ones from the provider (For example, you will use "anthropic.claude-3-5-sonnet-20240620-v1:0" when you're calling LiteLLM just like you would when calling Amazon Bedrock directly)
#In the case where there are model name conflicts, a prefix will be used (For example, the Azure and the openAI model names conflict, so when you are using Azure, you will use "azure/gpt-4o-realtime-preview-2024-10-01")

#Some model providers require additional user-specific configuration (such as Azure which requires you to specify your own api_base with your resource name, and your api_version). 
#In this case, the provider is commented out, and you should uncomment it and provide your specific info

#For more detailed information about each provider, refer to the docs: https://docs.litellm.ai/docs/providers

#If you are not interested in a particular provider, just remove it from your config.yaml, and redeploy, and it will no longer show up in your LiteLLM deployment

#If a particular provider is not working, double check your .env file, and make sure you have provided a valid api key for that provider, and then redeploy

model_list:

#Full details on guardrails here: https://docs.litellm.ai/docs/proxy/guardrails/bedrock
# guardrails:
#   - guardrail_name: "bedrock-pre-guard"
#     litellm_params:
#       guardrail: bedrock
#       mode: "during_call" # supported values: "pre_call", "post_call", "during_call"
#       guardrailIdentifier: ff6ujrregl1q # your guardrail ID on bedrock
#       guardrailVersion: "1"         # your guardrail version on bedrock
#       default_on: true # enforces the guardrail serverside for all models. Caller does not need to pass in the name of the guardrail for it to be enforced.

router_settings:
  routing_strategy: usage-based-routing-v2
  redis_host: os.environ/REDIS_HOST
  redis_port: os.environ/REDIS_PORT
  redis_password: os.environ/REDIS_PASSWORD
  enable_pre_call_check: true
  # context_window_fallbacks: [{"gpt-4": ["anthropic.claude-3-5-sonnet-20240620-v1:0"]}] #Configure fallbacks for context window exeeded errors (In this example, we will fall back to Claude Sonnet if over 8000 tokens, which is gpt-4's limit)
  # fallbacks: [{"gpt-4o": ["anthropic.claude-3-5-sonnet-20240620-v1:0"]}] #Configure fallbacks for any other error
  # default_fallbacks: ["anthropic.claude-3-haiku-20240307-v1:0"] #Configure fallbacks for any error for every model (the above fallback configurations override this one)

environment_variables:
  STORE_MODEL_IN_DB: 'True'
  LITELLM_LOG: "DEBUG"

litellm_settings:
  cache: True
  debug: True
  detailed_debug: True
  cache_params:
    type: redis
    host: os.environ/REDIS_HOST # Redis server hostname or IP address
    port: os.environ/REDIS_PORT # Redis server port (as a string)
    password: os.environ/REDIS_PASSWORD # Redis server password
    
  max_budget: 1000000000.0 # (float) sets max budget in dollars across the entire proxy across all API keys. Note, the budget does not apply to the master key. That is the only exception.
  budget_duration: 1mo # (str) frequency of budget reset - You can set duration as seconds ("30s"), minutes ("30m"), hours ("30h"), days ("30d"), months ("1mo").

  max_internal_user_budget: 1000000000.0 # (float) sets default budget in dollars for each internal user. (Doesn't apply to Admins. Doesn't apply to Teams. Doesn't apply to master key)
  internal_user_budget_duration: "1mo" # (str) frequency of budget reset - You can set duration as seconds ("30s"), minutes ("30m"), hours ("30h"), days ("30d"), months ("1mo").

  success_callback: ["s3"]
  failure_callback: ["s3"]
  service_callback: ["s3"]
  s3_callback_params:
    s3_bucket_name:
    s3_region_name:
  #type: redis-semantic
  #similarity_threshold: 0.8   # similarity threshold for semantic cache
  #redis_semantic_cache_embedding_model: text-embedding-ada-002 # only works with text-embedding-ada-002 for now... https://github.com/BerriAI/litellm/issues/4001

  #ttl: Optional[float]
  #default_in_memory_ttl: Optional[float]
  #default_in_redis_ttl: Optional[float]
